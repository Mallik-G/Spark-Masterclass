{
  "paragraphs": [
    {
      "text": "%md \n# (LAB 1) Preparation\n\n**Note:** Run this before any spark command in the notebook. Restart interpreter if necessary!\n",
      "dateUpdated": "Mar 7, 2016 9:14:40 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455728526585_2142553540",
      "id": "20160217-180206_320918471",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch1\u003e(LAB 1) Preparation\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e Run this before any spark command in the notebook. Restart interpreter if necessary!\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 17, 2016 6:02:06 PM",
      "dateStarted": "Mar 7, 2016 9:14:40 PM",
      "dateFinished": "Mar 7, 2016 9:14:40 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%dep\nz.reset()\nz.addRepo(\"Spark Packages Repo\").url(\"http://dl.bintray.com/spark-packages/maven\")\nz.load(\"com.databricks:spark-csv_2.10:1.3.0\")\nz.load(\"org.apache.spark:spark-streaming-kafka_2.10:1.6.0\")\n",
      "dateUpdated": "Mar 7, 2016 9:04:17 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455721677993_-632325202",
      "id": "20160217-160757_1663731196",
      "dateCreated": "Feb 17, 2016 4:07:57 PM",
      "dateStarted": "Mar 7, 2016 9:04:17 PM",
      "dateFinished": "Mar 7, 2016 9:04:23 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nCalling sc will initialize the executors (org.apache.spark.executor.CoarseGrainedExecutorBackend) via yarn, if Zeppelin is configured as \"yarn-client\"",
      "dateUpdated": "Mar 7, 2016 9:14:43 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455733189515_1381750602",
      "id": "20160217-191949_732404084",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eCalling sc will initialize the executors (org.apache.spark.executor.CoarseGrainedExecutorBackend) via yarn, if Zeppelin is configured as \u0026ldquo;yarn-client\u0026rdquo;\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 17, 2016 7:19:49 PM",
      "dateStarted": "Mar 7, 2016 9:14:43 PM",
      "dateFinished": "Mar 7, 2016 9:14:43 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\nprint(sc.version)",
      "dateUpdated": "Mar 7, 2016 9:06:31 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455733196253_1519121284",
      "id": "20160217-191956_841399617",
      "dateCreated": "Feb 17, 2016 7:19:56 PM",
      "dateStarted": "Mar 7, 2016 9:06:31 PM",
      "dateFinished": "Mar 7, 2016 9:06:31 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n# (LAB 2) Working with RDDs",
      "dateUpdated": "Mar 7, 2016 9:14:45 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455728596566_-1362616209",
      "id": "20160217-180316_454559233",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch1\u003e(LAB 2) Working with RDDs\u003c/h1\u003e\n"
      },
      "dateCreated": "Feb 17, 2016 6:03:16 PM",
      "dateStarted": "Mar 7, 2016 9:14:45 PM",
      "dateFinished": "Mar 7, 2016 9:14:45 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nA little convenience function to \n\n- collect random samples of RDDs and DataFrames\n- print RDDs and DataFrames in table form without switching to %sql by leveraging Zeppelin Display capabilities",
      "dateUpdated": "Mar 7, 2016 9:14:47 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456420991436_1540964611",
      "id": "20160225-182311_2086547517",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eA little convenience function to\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ecollect random samples of RDDs and DataFrames\u003c/li\u003e\n\u003cli\u003eprint RDDs and DataFrames in table form without switching to %sql by leveraging Zeppelin Display capabilities\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Feb 25, 2016 6:23:11 PM",
      "dateStarted": "Mar 7, 2016 9:14:47 PM",
      "dateFinished": "Mar 7, 2016 9:14:47 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\ndef pprint(data, num\u003d8, asTable\u003dFalse, columns\u003dNone, sampleRatio\u003dNone, seed\u003d42):\n\n    # If a sampleRatio is given, a random sample with given seed is selected\n    subset \u003d data.sample(False, fraction\u003dsampleRatio, seed\u003dseed) if sampleRatio else data\n\n    # If it is a DataFrame, convert rows to arras and extract headers\n    if \"rdd\" in dir(data): \n        columns \u003d subset.columns\n        subset \u003d subset.map(lambda row: row.asDict().values())\n        \n    # If num is -1 all records should be collected - avoid for big data ...\n    array \u003d subset.collect() if num \u003d\u003d -1 else subset.take(num)\n    \n    # If asTable is True, sql format with columns c0, c1, ... as output\n    # If columns is array of column names, sql format with given columns as output\n    if asTable or columns:\n        output \u003d \"\"\n        for d in array:\n            l \u003d len(d)\n            output +\u003d \"\\t\".join([str(x) for x in d]) + \"\\n\"\n        if columns:\n            header \u003d \"\\t\".join([h for h in columns]) + \"\\n\" \n        else:\n            header \u003d \"\\t\".join([\"c%0d\" %i for i in range(l) ]) + \"\\n\"\n        print \"%table \" + header + output\n    else:\n        for d in array:\n            print d\n",
      "dateUpdated": "Mar 7, 2016 9:06:48 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "lineNumbers": false,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455730928852_1180141655",
      "id": "20160217-184208_572421563",
      "dateCreated": "Feb 17, 2016 6:42:08 PM",
      "dateStarted": "Mar 7, 2016 9:06:48 PM",
      "dateFinished": "Mar 7, 2016 9:06:48 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## 2.1 Create a simple RDD and sum up rows ",
      "dateUpdated": "Mar 7, 2016 9:14:51 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455730365406_-1911741022",
      "id": "20160217-183245_1834168142",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003e2.1 Create a simple RDD and sum up rows\u003c/h2\u003e\n"
      },
      "dateCreated": "Feb 17, 2016 6:32:45 PM",
      "dateStarted": "Mar 7, 2016 9:14:51 PM",
      "dateFinished": "Mar 7, 2016 9:14:51 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\nimport random\n\ndata \u003d [ [random.randint(10,99) for col in range(4)] for row in range(10)]\n\nrdd \u003d sc.parallelize(data, 4)\n\npprint(rdd, -1)",
      "dateUpdated": "Mar 7, 2016 9:06:53 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "c0",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "c1",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "c0",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "c1",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "editorMode": "ace/mode/scala",
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455728685323_1785116355",
      "id": "20160217-180445_1950689137",
      "dateCreated": "Feb 17, 2016 6:04:45 PM",
      "dateStarted": "Mar 7, 2016 9:06:53 PM",
      "dateFinished": "Mar 7, 2016 9:06:55 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\nm \u003d rdd.map(lambda x: sum(x))\nprint(m.collect())\n\ns \u003d m.reduce(lambda x,y: x + y)\nprint \"total \u003d \", s\n",
      "dateUpdated": "Mar 7, 2016 9:06:57 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455729974344_1711270805",
      "id": "20160217-182614_514005626",
      "dateCreated": "Feb 17, 2016 6:26:14 PM",
      "dateStarted": "Mar 7, 2016 9:06:57 PM",
      "dateFinished": "Mar 7, 2016 9:06:59 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n## 2.2 Load the famous Iris data from HDFS and so some basic calculations\n\nAttribute Information:\n\n    [0] sepal length in cm\n    [1] sepal width in cm\n    [2] petal length in cm\n    [3] petal width in cm\n    [4] class: Iris Setosa, Iris Versicolour, Iris Virginica\n",
      "dateUpdated": "Mar 7, 2016 9:14:53 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455730414140_-594083308",
      "id": "20160217-183334_447701235",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003e2.2 Load the famous Iris data from HDFS and so some basic calculations\u003c/h2\u003e\n\u003cp\u003eAttribute Information:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[0] sepal length in cm\n[1] sepal width in cm\n[2] petal length in cm\n[3] petal width in cm\n[4] class: Iris Setosa, Iris Versicolour, Iris Virginica\n\u003c/code\u003e\u003c/pre\u003e\n"
      },
      "dateCreated": "Feb 17, 2016 6:33:34 PM",
      "dateStarted": "Mar 7, 2016 9:14:53 PM",
      "dateFinished": "Mar 7, 2016 9:14:53 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\ndef split(row):\n    parts \u003d row.split(\",\")\n    return [float(v) for v in parts[:4]] + [parts[4]]\n    \nfile \u003d sc.textFile(\"/tmp/iris.data\")\n\n# remove empty lines and split each line\niris \u003d file.filter(lambda row: len(row)\u003e0)\\\n           .map(split)\n\nprint iris.count()\n",
      "dateUpdated": "Mar 7, 2016 9:07:04 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "sepL",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "sepW",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "sepL",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "sepW",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "editorMode": "ace/mode/scala",
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455730410574_1910262607",
      "id": "20160217-183330_850886049",
      "dateCreated": "Feb 17, 2016 6:33:30 PM",
      "dateStarted": "Mar 7, 2016 9:07:04 PM",
      "dateFinished": "Mar 7, 2016 9:07:04 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\npprint(iris, sampleRatio\u003d0.1, num\u003d-1, columns\u003d[\"sepL\", \"sepW\", \"petL\", \"petW\", \"species\"])\n",
      "dateUpdated": "Mar 7, 2016 9:07:06 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "sepL",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "sepW",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "sepL",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "sepW",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "editorMode": "ace/mode/scala",
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456422059549_477238407",
      "id": "20160225-184059_236051655",
      "dateCreated": "Feb 25, 2016 6:40:59 PM",
      "dateStarted": "Mar 7, 2016 9:07:06 PM",
      "dateFinished": "Mar 7, 2016 9:07:06 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nCalculate average sepal length per class",
      "dateUpdated": "Mar 7, 2016 9:14:56 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455732727584_1239523204",
      "id": "20160217-191207_1811926142",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eCalculate average sepal length per class\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 17, 2016 7:12:07 PM",
      "dateStarted": "Mar 7, 2016 9:14:56 PM",
      "dateFinished": "Mar 7, 2016 9:14:56 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\ntuples \u003d iris.map(lambda row: [row[4], row[0]]) \n\nresult \u003d tuples.groupByKey().mapValues(lambda row: sum(row)/len(row))\n\npprint(result)",
      "dateUpdated": "Mar 7, 2016 9:07:11 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455731053446_174159554",
      "id": "20160217-184413_920390523",
      "dateCreated": "Feb 17, 2016 6:44:13 PM",
      "dateStarted": "Mar 7, 2016 9:07:11 PM",
      "dateFinished": "Mar 7, 2016 9:07:11 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n# (LAB 3) Working with DataFrames converted from RDDs",
      "dateUpdated": "Mar 7, 2016 9:14:58 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455733095161_-1189880398",
      "id": "20160217-191815_2111213829",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch1\u003e(LAB 3) Working with DataFrames converted from RDDs\u003c/h1\u003e\n"
      },
      "dateCreated": "Feb 17, 2016 7:18:15 PM",
      "dateStarted": "Mar 7, 2016 9:14:58 PM",
      "dateFinished": "Mar 7, 2016 9:14:58 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n## 3.1 Transform Iris RDD to DataFrame",
      "dateUpdated": "Mar 7, 2016 9:15:00 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455733606592_736892833",
      "id": "20160217-192646_1655733180",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003e3.1 Transform Iris RDD to DataFrame\u003c/h2\u003e\n"
      },
      "dateCreated": "Feb 17, 2016 7:26:46 PM",
      "dateStarted": "Mar 7, 2016 9:15:00 PM",
      "dateFinished": "Mar 7, 2016 9:15:00 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\nfrom pyspark.sql.types import *\n\nschema \u003d StructType([ \\\n   StructField(\"sepalLength\", DoubleType(),  True), \\\n   StructField(\"sepalWidth\",  DoubleType(),  True), \\\n   StructField(\"PetalLength\", DoubleType(),  True), \\\n   StructField(\"PetalWidth\",  DoubleType(),  True), \\\n   StructField(\"Class\",       StringType(),  True)\n])\n\nirisDf \u003d sqlContext.createDataFrame(iris, schema\u003dschema)\n\nsqlContext.registerDataFrameAsTable(irisDf, \"Iris\")\n\nirisDf.show()",
      "dateUpdated": "Mar 7, 2016 9:07:15 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455733137494_-827370313",
      "id": "20160217-191857_674202811",
      "dateCreated": "Feb 17, 2016 7:18:57 PM",
      "dateStarted": "Mar 7, 2016 9:07:15 PM",
      "dateFinished": "Mar 7, 2016 9:07:16 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\nirisDf.select([\"Class\", \"sepalLength\"]).groupBy(\"Class\").avg(\"sepalLength\").show()",
      "dateUpdated": "Mar 7, 2016 9:07:20 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455733142141_535025542",
      "id": "20160217-191902_1601893514",
      "dateCreated": "Feb 17, 2016 7:19:02 PM",
      "dateStarted": "Mar 7, 2016 9:07:20 PM",
      "dateFinished": "Mar 7, 2016 9:07:21 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql\n\nselect Class, avg(sepalLength) as avgSL\nfrom Iris\ngroup by Class",
      "dateUpdated": "Mar 7, 2016 9:07:22 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "multiBarChart",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "Class",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "avgSL",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "Class",
              "index": 0.0,
              "aggr": "sum"
            }
          }
        },
        "editorMode": "ace/mode/sql",
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455734701542_-4767895",
      "id": "20160217-194501_913187085",
      "dateCreated": "Feb 17, 2016 7:45:01 PM",
      "dateStarted": "Mar 7, 2016 9:07:22 PM",
      "dateFinished": "Mar 7, 2016 9:07:24 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \n\n# (LAB 4) Analyze World Development Indicators \n\n## 4.1 Load World Development Indicators from HDFS as DataFrame\n\n**Data source:** https://www.kaggle.com/worldbank/world-development-indicators/downloads/world-development-indicators-release-2016-01-28-06-31-53.zip\n\n**Changes:** Reduced to a subset with countries of the European Union only  (AUT, BEL, BUL, CYP, CZE, DEU, DNK, ESP, EST, FIN, FRA, GBR, GRC, HRV, HUN, IRL, ITA, LTU, LUX, LVA, MLT, NLD, POL, PRT, ROM, SVK, SVN, SWE) ",
      "dateUpdated": "Mar 7, 2016 9:15:03 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455721677992_-631940453",
      "id": "20160217-160757_172602457",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch1\u003e(LAB 4) Analyze World Development Indicators\u003c/h1\u003e\n\u003ch2\u003e4.1 Load World Development Indicators from HDFS as DataFrame\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eData source:\u003c/strong\u003e https://www.kaggle.com/worldbank/world-development-indicators/downloads/world-development-indicators-release-2016-01-28-06-31-53.zip\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eChanges:\u003c/strong\u003e Reduced to a subset with countries of the European Union only  (AUT, BEL, BUL, CYP, CZE, DEU, DNK, ESP, EST, FIN, FRA, GBR, GRC, HRV, HUN, IRL, ITA, LTU, LUX, LVA, MLT, NLD, POL, PRT, ROM, SVK, SVN, SWE)\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 17, 2016 4:07:57 PM",
      "dateStarted": "Mar 7, 2016 9:15:03 PM",
      "dateFinished": "Mar 7, 2016 9:15:03 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\n\nhdfs dfs -ls /tmp/europe-indicators.csv",
      "dateUpdated": "Mar 7, 2016 9:07:30 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/sh",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455721677994_-631170955",
      "id": "20160217-160757_474000621",
      "dateCreated": "Feb 17, 2016 4:07:57 PM",
      "dateStarted": "Mar 7, 2016 9:07:30 PM",
      "dateFinished": "Mar 7, 2016 9:07:33 PM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nfrom pyspark.sql.types import *\n\nschema \u003d StructType([ \\\n   StructField(\"CountryName\",   StringType(),  True), \\\n   StructField(\"CountryCode\",   StringType(),  True), \\\n   StructField(\"IndicatorName\", StringType(),  True), \\\n   StructField(\"IndicatorCode\", StringType(),  True), \\\n   StructField(\"Year\",          IntegerType(), True), \\\n   StructField(\"Value\",         DoubleType(),  True)  \\\n])\n\nindicators_csv \u003d sqlContext.read.load(\u0027/tmp/europe-indicators.csv\u0027, format\u003d\u0027com.databricks.spark.csv\u0027, header\u003d\u0027true\u0027, schema\u003dschema).cache()\nsqlContext.registerDataFrameAsTable(indicators_csv, \"IndicatorsRDD\")\n\nprint(indicators_csv.count())\n\n",
      "dateUpdated": "Mar 7, 2016 9:09:14 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455721677994_-631170955",
      "id": "20160217-160757_227211711",
      "dateCreated": "Feb 17, 2016 4:07:57 PM",
      "dateStarted": "Mar 7, 2016 9:09:14 PM",
      "dateFinished": "Mar 7, 2016 9:09:17 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nLet\u0027s look at the schema of the Indicators table",
      "dateUpdated": "Mar 7, 2016 9:15:06 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455721677994_-631170955",
      "id": "20160217-160757_324524249",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eLet\u0027s look at the schema of the Indicators table\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 17, 2016 4:07:57 PM",
      "dateStarted": "Mar 7, 2016 9:15:06 PM",
      "dateFinished": "Mar 7, 2016 9:15:06 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nindicators_csv.printSchema()\nindicators_csv.sample(False, 0.1).show()",
      "dateUpdated": "Mar 7, 2016 9:09:21 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455721677994_-631170955",
      "id": "20160217-160757_785447344",
      "dateCreated": "Feb 17, 2016 4:07:57 PM",
      "dateStarted": "Mar 7, 2016 9:09:21 PM",
      "dateFinished": "Mar 7, 2016 9:09:21 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nCode/value encoding is not that optimal ... Let\u0027s transform the data set and store the result os ORC\n",
      "dateUpdated": "Mar 7, 2016 9:15:08 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455721677994_-631555704",
      "id": "20160217-160757_938348355",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eCode/value encoding is not that optimal \u0026hellip; Let\u0027s transform the data set and store the result os ORC\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 17, 2016 4:07:57 PM",
      "dateStarted": "Mar 7, 2016 9:15:08 PM",
      "dateFinished": "Mar 7, 2016 9:15:08 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n## 4.2 Transform Indicators table to Columns \n",
      "dateUpdated": "Mar 7, 2016 9:15:10 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455721677995_-631555704",
      "id": "20160217-160757_1836277504",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003e4.2 Transform Indicators table to Columns\u003c/h2\u003e\n"
      },
      "dateCreated": "Feb 17, 2016 4:07:57 PM",
      "dateStarted": "Mar 7, 2016 9:15:10 PM",
      "dateFinished": "Mar 7, 2016 9:15:10 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nSpark 1.5 does not provide a `pivot` method for DataFrames, hence we need to write our own pivot via RDDs and `aggregateByKey`\n\nSome caveats for this step:\n- Return a row from `merge`, python dictionaries are deprecated\n- `**value` is a nice trick to convert a dictionary to a keyword parameter list (Rows are unmutable)\n- Initialize with all indicators and set them to None\n- `.`are not allowed in column names, so replace with `_`",
      "dateUpdated": "Mar 7, 2016 9:15:12 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455721677995_-631555704",
      "id": "20160217-160757_1078973287",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eSpark 1.5 does not provide a \u003ccode\u003epivot\u003c/code\u003e method for DataFrames, hence we need to write our own pivot via RDDs and \u003ccode\u003eaggregateByKey\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eSome caveats for this step:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eReturn a row from \u003ccode\u003emerge\u003c/code\u003e, python dictionaries are deprecated\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e**value\u003c/code\u003e is a nice trick to convert a dictionary to a keyword parameter list (Rows are unmutable)\u003c/li\u003e\n\u003cli\u003eInitialize with all indicators and set them to None\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e.\u003c/code\u003eare not allowed in column names, so replace with \u003ccode\u003e_\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Feb 17, 2016 4:07:57 PM",
      "dateStarted": "Mar 7, 2016 9:15:12 PM",
      "dateFinished": "Mar 7, 2016 9:15:12 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\ncolumns \u003d indicators_csv.map(lambda row: row.IndicatorCode.replace(\".\", \"_\")).distinct().collect()\nbc \u003d sc.broadcast(columns)\n\ndef seq(u, v):\n    if u \u003d\u003d None: \n        u \u003d {ind: None for ind in bc.value}          # Use value of broadcast variable to initialize the dictionary and ensure all rows have all indicators\n    u[v.IndicatorCode.replace(\".\",\"_\")] \u003d v.Value    # Set this indicators value converted to float\n    return u\n\ndef comb(u1, u2):\n    u1.update(u2)\n    return u1\n\ndef merge(keys, value):\n    value[\"Country\"] \u003d keys[0]\n    value[\"Year\"] \u003d int(keys[1])\n    return Row(**value)\n\ndata \u003d indicators_csv.select([\"CountryCode\", \"IndicatorCode\", \"Year\", \"Value\"])\\\n                     .rdd\\\n                     .keyBy(lambda row: row.CountryCode + \"|\" + str(row.Year))\\\n                     .aggregateByKey(None, seq, comb)\\\n                     .map(lambda tuple: merge(tuple[0].split(\"|\"), tuple[1]))\\\n                     .cache()\n\n",
      "dateUpdated": "Mar 7, 2016 9:09:31 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455721677995_-631555704",
      "id": "20160217-160757_1769713575",
      "dateCreated": "Feb 17, 2016 4:07:57 PM",
      "dateStarted": "Mar 7, 2016 9:09:31 PM",
      "dateFinished": "Mar 7, 2016 9:09:34 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nFinally, transform RDD back to DataFrame and register a table with the hiveContext (due to ORC)\n\n**Notes:**\n\n- The StructType schema **has to be sorted!** Spark does not match schema names with Row column names but uses the order of elements in Row and schema to apply types\n- Also, due to the many null values, automatic schema inference will only work properly when \"samplingRatio\u003d100\" in createDataFrame. However, I wouldn\u0027t rely on it ...",
      "dateUpdated": "Mar 7, 2016 9:15:15 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455721677995_-631555704",
      "id": "20160217-160757_774386140",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eFinally, transform RDD back to DataFrame and register a table with the hiveContext (due to ORC)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNotes:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe StructType schema \u003cstrong\u003ehas to be sorted!\u003c/strong\u003e Spark does not match schema names with Row column names but uses the order of elements in Row and schema to apply types\u003c/li\u003e\n\u003cli\u003eAlso, due to the many null values, automatic schema inference will only work properly when \u0026ldquo;samplingRatio\u003d100\u0026rdquo; in createDataFrame. However, I wouldn\u0027t rely on it \u0026hellip;\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Feb 17, 2016 4:07:57 PM",
      "dateStarted": "Mar 7, 2016 9:15:15 PM",
      "dateFinished": "Mar 7, 2016 9:15:15 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\nfrom pyspark.sql.types import *\n\nsqlContext.setConf(\"spark.sql.orc.filterPushdown\", \"true\")\n\nfields \u003d [StructField(ind, DoubleType(), True) for ind in columns ] + \\\n         [StructField(\"Year\", IntegerType(), False), StructField(\"Country\", StringType(), False)]\nsortedFields \u003d sorted(fields, key\u003dlambda x: x.name)\nsortedSchema \u003d StructType(fields\u003dsortedFields)\n\nindicators \u003d sqlContext.createDataFrame(data, schema \u003d sortedSchema)\nsqlContext.registerDataFrameAsTable(indicators, \"Indicators\")",
      "dateUpdated": "Mar 7, 2016 9:09:37 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455721677995_-631555704",
      "id": "20160217-160757_1498734662",
      "dateCreated": "Feb 17, 2016 4:07:57 PM",
      "dateStarted": "Mar 7, 2016 9:09:37 PM",
      "dateFinished": "Mar 7, 2016 9:09:38 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n## 4.3 Save transformed table as ORC",
      "dateUpdated": "Mar 7, 2016 9:15:20 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455721677999_-633094700",
      "id": "20160217-160757_1019552859",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003e4.3 Save transformed table as ORC\u003c/h2\u003e\n"
      },
      "dateCreated": "Feb 17, 2016 4:07:57 PM",
      "dateStarted": "Mar 7, 2016 9:15:20 PM",
      "dateFinished": "Mar 7, 2016 9:15:20 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\nhdfs dfs -rm -r /tmp/europe-indicators_transformed_orc\n",
      "dateUpdated": "Mar 7, 2016 9:09:42 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/sh",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455721677999_-633094700",
      "id": "20160217-160757_1122870613",
      "dateCreated": "Feb 17, 2016 4:07:57 PM",
      "dateStarted": "Mar 7, 2016 9:09:42 PM",
      "dateFinished": "Mar 7, 2016 9:09:43 PM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\nindicators.write.orc(\"/tmp/europe-indicators_transformed_orc\")\n",
      "dateUpdated": "Mar 7, 2016 9:09:46 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455721678000_-622706479",
      "id": "20160217-160758_1399156780",
      "dateCreated": "Feb 17, 2016 4:07:58 PM",
      "dateStarted": "Mar 7, 2016 9:09:46 PM",
      "dateFinished": "Mar 7, 2016 9:13:50 PM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## 4.4 Some simple Queries",
      "dateUpdated": "Mar 7, 2016 9:15:23 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455721678001_-623091228",
      "id": "20160217-160758_1209691520",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003e4.4 Some simple Queries\u003c/h2\u003e\n"
      },
      "dateCreated": "Feb 17, 2016 4:07:58 PM",
      "dateStarted": "Mar 7, 2016 9:15:23 PM",
      "dateFinished": "Mar 7, 2016 9:15:23 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nLoad ORC data again to benefit from predicate pushdow, etc",
      "dateUpdated": "Mar 7, 2016 9:15:25 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455721678000_-622706479",
      "id": "20160217-160758_1485544919",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eLoad ORC data again to benefit from predicate pushdow, etc\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 17, 2016 4:07:58 PM",
      "dateStarted": "Mar 7, 2016 9:15:25 PM",
      "dateFinished": "Mar 7, 2016 9:15:25 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\nindicators_t \u003d sqlContext.read.orc(\"/tmp/europe-indicators_transformed_orc\")\nsqlContext.registerDataFrameAsTable(indicators_t, \"Indicators_t\")\nsqlContext.cacheTable(\"Indicators_t\")\n",
      "dateUpdated": "Feb 22, 2016 7:32:15 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455721678000_-622706479",
      "id": "20160217-160758_546796282",
      "dateCreated": "Feb 17, 2016 4:07:58 PM",
      "dateStarted": "Feb 22, 2016 7:32:15 PM",
      "dateFinished": "Feb 22, 2016 7:32:16 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nExecute some queries",
      "dateUpdated": "Mar 7, 2016 9:15:28 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455735006822_383131805",
      "id": "20160217-195006_1167058578",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eExecute some queries\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 17, 2016 7:50:06 PM",
      "dateStarted": "Mar 7, 2016 9:15:28 PM",
      "dateFinished": "Mar 7, 2016 9:15:28 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql\n\n-- SP.DYN.CBRT.IN: Birth rate, crude (per 1,000 people)\n\nselect Country, Year, SP_DYN_CBRT_IN from Indicators_t\nwhere Country in (\u0027AUT\u0027, \u0027FRA\u0027, \u0027DEU\u0027, \u0027GRC\u0027, \u0027IRL\u0027, \u0027ITA\u0027, \u0027NLD\u0027, \u0027PRT\u0027, \u0027ESP\u0027, \u0027GBR\u0027) \n  and Year \u003e 1990\norder by Country, Year\n",
      "dateUpdated": "Feb 22, 2016 7:32:20 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "graph": {
          "mode": "multiBarChart",
          "height": 300.0,
          "optionOpen": true,
          "keys": [
            {
              "name": "Country",
              "index": 0.0,
              "aggr": "sum",
              "$$hashKey": "object:1876"
            }
          ],
          "values": [
            {
              "name": "SP_DYN_CBRT_IN",
              "index": 2.0,
              "aggr": "sum",
              "$$hashKey": "object:1880"
            }
          ],
          "groups": [
            {
              "name": "Year",
              "index": 1.0,
              "aggr": "sum",
              "$$hashKey": "object:1878"
            }
          ],
          "scatter": {
            "yAxis": {
              "name": "Year",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455721678001_-623091228",
      "id": "20160217-160758_635379121",
      "dateCreated": "Feb 17, 2016 4:07:58 PM",
      "dateStarted": "Feb 22, 2016 7:32:20 PM",
      "dateFinished": "Feb 22, 2016 7:32:22 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql\n\n-- SL.UEM.1524.NE.ZS: Unemployment, youth total (% of total labor force ages 15-24) (national estimate)\n\nselect Country, Year, SL_UEM_1524_NE_ZS from Indicators_t\nwhere Country in (\u0027AUT\u0027, \u0027FRA\u0027, \u0027DEU\u0027, \u0027GRC\u0027, \u0027IRL\u0027, \u0027ITA\u0027, \u0027NLD\u0027, \u0027PRT\u0027, \u0027ESP\u0027, \u0027GBR\u0027) \n  and Year \u003e 1990\norder by Country, Year\n\n",
      "dateUpdated": "Feb 22, 2016 7:32:28 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "graph": {
          "mode": "multiBarChart",
          "height": 300.0,
          "optionOpen": true,
          "keys": [
            {
              "name": "Country",
              "index": 0.0,
              "aggr": "sum",
              "$$hashKey": "object:1033"
            }
          ],
          "values": [
            {
              "name": "SL_UEM_1524_NE_ZS",
              "index": 2.0,
              "aggr": "sum",
              "$$hashKey": "object:1037"
            }
          ],
          "groups": [
            {
              "name": "Year",
              "index": 1.0,
              "aggr": "sum",
              "$$hashKey": "object:1035"
            }
          ],
          "scatter": {
            "yAxis": {
              "name": "Year",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455721678001_-623091228",
      "id": "20160217-160758_557730296",
      "dateCreated": "Feb 17, 2016 4:07:58 PM",
      "dateStarted": "Feb 22, 2016 7:32:28 PM",
      "dateFinished": "Feb 22, 2016 7:32:31 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql\n\n-- SL.UEM.1524.NE.ZS: Unemployment, youth total (% of total labor force ages 15-24) (national estimate)\n-- SL.UEM.TOTL.NE.ZS: Unemployment, total (% of total labor force) (national estimate)\n-- SP.DYN.CBRT.IN: Birth rate, crude (per 1,000 people)\n\nselect Country, Year, SL_UEM_1524_NE_ZS, SP_DYN_CBRT_IN  from Indicators_t\nwhere Country in (\u0027AUT\u0027, \u0027FRA\u0027, \u0027DEU\u0027, \u0027GRC\u0027, \u0027IRL\u0027, \u0027ITA\u0027, \u0027NLD\u0027, \u0027PRT\u0027, \u0027ESP\u0027, \u0027GBR\u0027)\n  and Year \u003e 1990\n  and Year \u003c 2015\norder by Country, Year\n",
      "dateUpdated": "Feb 22, 2016 7:32:34 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "graph": {
          "mode": "scatterChart",
          "height": 300.0,
          "optionOpen": true,
          "keys": [
            {
              "name": "Country",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "Year",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "Year",
              "index": 1.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "SL_UEM_1524_NE_ZS",
              "index": 2.0,
              "aggr": "sum"
            },
            "group": {
              "name": "Country",
              "index": 0.0,
              "aggr": "sum"
            },
            "size": {
              "name": "SP_DYN_CBRT_IN",
              "index": 3.0,
              "aggr": "sum"
            }
          }
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455721678002_-621936982",
      "id": "20160217-160758_983953287",
      "dateCreated": "Feb 17, 2016 4:07:58 PM",
      "dateStarted": "Feb 22, 2016 7:32:35 PM",
      "dateFinished": "Feb 22, 2016 7:32:37 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Optional: Pure SQL approach\n\nOf course, this result could have been calculated without pivoting the table",
      "dateUpdated": "Mar 7, 2016 9:15:32 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455721678002_-621936982",
      "id": "20160217-160758_558424894",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003eOptional: Pure SQL approach\u003c/h2\u003e\n\u003cp\u003eOf course, this result could have been calculated without pivoting the table\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 17, 2016 4:07:58 PM",
      "dateStarted": "Mar 7, 2016 9:15:32 PM",
      "dateFinished": "Mar 7, 2016 9:15:32 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\nsqlContext.registerDataFrameAsTable(indicators_csv, \"Indicators\")",
      "dateUpdated": "Feb 22, 2016 7:32:40 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455721678003_-622321730",
      "id": "20160217-160758_2082947123",
      "dateCreated": "Feb 17, 2016 4:07:58 PM",
      "dateStarted": "Feb 22, 2016 7:32:40 PM",
      "dateFinished": "Feb 22, 2016 7:32:41 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql\n\nselect Year, CountryCode, max(SL) as UNEM, max(SP) as CBRT from\n  (select Year, CountryCode, \n          case IndicatorCode when \u0027SP.DYN.CBRT.IN\u0027  then max(Value) else NULL end as SP,\n          case IndicatorCode when \u0027SL.UEM.1524.NE.ZS\u0027 then max(Value) else NULL end as SL\n   from Indicators\n   where IndicatorCode in (\u0027SP.DYN.CBRT.IN\u0027, \u0027SL.UEM.1524.NE.ZS\u0027) \n     and CountryCode in (\u0027AUT\u0027, \u0027FRA\u0027, \u0027DEU\u0027, \u0027GRC\u0027, \u0027IRL\u0027, \u0027ITA\u0027, \u0027NLD\u0027, \u0027PRT\u0027, \u0027ESP\u0027, \u0027GBR\u0027) \n     and year \u003e 1990\n   group by Year, CountryCode, IndicatorCode\n   order by Year, CountryCode\n  ) Indicators2\ngroup by CountryCode, Year",
      "dateUpdated": "Feb 22, 2016 7:32:43 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/sql",
        "graph": {
          "mode": "scatterChart",
          "height": 300.0,
          "optionOpen": true,
          "keys": [
            {
              "name": "Year",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "CountryCode",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [
            {
              "name": "UNEM",
              "index": 2.0,
              "aggr": "sum"
            }
          ],
          "scatter": {
            "xAxis": {
              "name": "Year",
              "index": 0.0,
              "aggr": "sum"
            },
            "group": {
              "name": "CountryCode",
              "index": 1.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "UNEM",
              "index": 2.0,
              "aggr": "sum"
            },
            "size": {
              "name": "CBRT",
              "index": 3.0,
              "aggr": "sum"
            }
          }
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455721678003_-622321730",
      "id": "20160217-160758_797292486",
      "dateCreated": "Feb 17, 2016 4:07:58 PM",
      "dateStarted": "Feb 22, 2016 7:32:43 PM",
      "dateFinished": "Feb 22, 2016 7:33:05 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n# (Lab 5) Clustering with Spark ML\n\n## 5.1 Select relevant data\n",
      "dateUpdated": "Mar 7, 2016 9:15:35 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455721678004_-624245475",
      "id": "20160217-160758_1878689418",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch1\u003e(Lab 5) Clustering with Spark ML\u003c/h1\u003e\n\u003ch2\u003e5.1 Select relevant data\u003c/h2\u003e\n"
      },
      "dateCreated": "Feb 17, 2016 4:07:58 PM",
      "dateStarted": "Mar 7, 2016 9:15:35 PM",
      "dateFinished": "Mar 7, 2016 9:15:35 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\ndef cvtCodes(code):\n    return code.lower().replace(\".\", \"_\")\n    \nfeatures \u003d [\n    cvtCodes(c) for c in [\n        \"SL.UEM.1524.NE.ZS\",   # Unemployment, youth total (% of total labor force ages 15-24) (national estimate)\n        \"GC.BAL.CASH.GD.ZS\",   # Cash surplus/deficit (% of GDP)\n        \"FP.CPI.TOTL.ZG\"       # Inflation, consumer prices (annual %) \n    ]\n]\n\nyears \u003d [2007, 2008, 2009, 2010, 2011, 2012]\neu \u003d indicators_t[indicators_t.Year.isin(years)].select([\"country\", \"year\"] + features)\n\nsqlContext.registerDataFrameAsTable(eu, \"eu\")\n\n",
      "dateUpdated": "Mar 7, 2016 9:14:35 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455881276661_1801178350",
      "id": "20160219-122756_1968678957",
      "dateCreated": "Feb 19, 2016 12:27:56 PM",
      "dateStarted": "Feb 22, 2016 7:33:19 PM",
      "dateFinished": "Feb 22, 2016 7:33:25 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n## 5.2 Create KMeans Pipeline\n\nNote: Input columns (features) need to be in `Vector` format. `pyspark.ml.feature.VectorAssembler` allows to pipeline this",
      "dateUpdated": "Mar 7, 2016 9:15:38 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455961002286_120651010",
      "id": "20160220-103642_987475161",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003e5.2 Create KMeans Pipeline\u003c/h2\u003e\n\u003cp\u003eNote: Input columns (features) need to be in \u003ccode\u003eVector\u003c/code\u003e format. \u003ccode\u003epyspark.ml.feature.VectorAssembler\u003c/code\u003e allows to pipeline this\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 20, 2016 10:36:42 AM",
      "dateStarted": "Mar 7, 2016 9:15:38 PM",
      "dateFinished": "Mar 7, 2016 9:15:38 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.clustering import KMeans, KMeansModel\nfrom pyspark.ml import Pipeline\n\nassembler \u003d VectorAssembler(inputCols\u003dfeatures, outputCol\u003d\"features\")\n\nkmeans \u003d KMeans(k\u003d3, seed\u003d42)\n\npipeline \u003d Pipeline(stages\u003d[assembler, kmeans])\nmodel \u003d pipeline.fit(eu)\n\ntransformed \u003d model.transform(eu).select(\"country\", \"year\", \"prediction\").sort([\"country\", \"year\"])\nsqlContext.registerDataFrameAsTable(transformed, \"Classes\")\n",
      "dateUpdated": "Feb 22, 2016 7:33:27 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455951871065_-873260058",
      "id": "20160220-080431_669137345",
      "dateCreated": "Feb 20, 2016 8:04:31 AM",
      "dateStarted": "Feb 22, 2016 7:33:27 PM",
      "dateFinished": "Feb 22, 2016 7:33:37 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n## 5.3 Visualize Countries in classes over years\n\n**Caveat**: The classes are **not** to interprete as an ordered list, they are complete random!",
      "dateUpdated": "Mar 7, 2016 9:15:41 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456088799423_1056112402",
      "id": "20160221-220639_370088719",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003e5.3 Visualize Countries in classes over years\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eCaveat\u003c/strong\u003e: The classes are \u003cstrong\u003enot\u003c/strong\u003e to interprete as an ordered list, they are complete random!\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 21, 2016 10:06:39 PM",
      "dateStarted": "Mar 7, 2016 9:15:41 PM",
      "dateFinished": "Mar 7, 2016 9:15:41 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql\n\nselect country, year,  prediction + 1 as class\nfrom Classes \norder by country, year\n",
      "dateUpdated": "Feb 22, 2016 7:33:40 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "multiBarChart",
          "height": 300.0,
          "optionOpen": true,
          "keys": [
            {
              "name": "country",
              "index": 0.0,
              "aggr": "sum",
              "$$hashKey": "object:1190"
            }
          ],
          "values": [
            {
              "name": "class",
              "index": 2.0,
              "aggr": "sum",
              "$$hashKey": "object:1194"
            }
          ],
          "groups": [
            {
              "name": "year",
              "index": 1.0,
              "aggr": "sum",
              "$$hashKey": "object:1192"
            }
          ],
          "scatter": {
            "xAxis": {
              "name": "year",
              "index": 1.0,
              "aggr": "sum"
            },
            "group": {
              "name": "country",
              "index": 0.0,
              "aggr": "sum"
            }
          },
          "lineWithFocus": true,
          "forceY": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455882220304_-1609273651",
      "id": "20160219-124340_227831581",
      "dateCreated": "Feb 19, 2016 12:43:40 PM",
      "dateStarted": "Feb 22, 2016 7:33:40 PM",
      "dateFinished": "Feb 22, 2016 7:33:43 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n## 5.4 Show countries per year and class as lists\n\nNote: There is no function for `GroupedData` to collect values as list. Hence, back to `RDD` and `aggregateByKey`",
      "dateUpdated": "Mar 7, 2016 9:15:43 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456088896022_-1205336008",
      "id": "20160221-220816_635805135",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003e5.4 Show countries per year and class as lists\u003c/h2\u003e\n\u003cp\u003eNote: There is no function for \u003ccode\u003eGroupedData\u003c/code\u003e to collect values as list. Hence, back to \u003ccode\u003eRDD\u003c/code\u003e and \u003ccode\u003eaggregateByKey\u003c/code\u003e\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 21, 2016 10:08:16 PM",
      "dateStarted": "Mar 7, 2016 9:15:43 PM",
      "dateFinished": "Mar 7, 2016 9:15:44 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\ndef seq(u, v):\n    if u \u003d\u003d None: u \u003d []\n    u.append(v.country)\n    return u\n\ndef comb(u1, u2):\n    return u1 + u2\n\ndata \u003d transformed.select([\"year\", \"country\", \"prediction\"])\\\n                  .rdd\\\n                  .keyBy(lambda row: str(row.year) + \":\" + str(row.prediction))\\\n                  .aggregateByKey(None, seq, comb)\\\n                  .sortByKey()\\\n                  .map(lambda tuple: (tuple[0], \", \".join(sorted(tuple[1]))))\n\nyear \u003d \"\"\nfor c in data.collect():\n    y, cl \u003d c[0].split(\":\")\n    if y !\u003d year:\n        print \"\\nYear:\", y\n        year \u003d y\n    print cl, \"\u003d\", c[1]\n\n",
      "dateUpdated": "Feb 22, 2016 7:33:47 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456057832372_-24834143",
      "id": "20160221-133032_407492549",
      "dateCreated": "Feb 21, 2016 1:30:32 PM",
      "dateStarted": "Feb 22, 2016 7:33:47 PM",
      "dateFinished": "Feb 22, 2016 7:34:01 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Little helper for indicator codes",
      "dateUpdated": "Mar 7, 2016 9:15:47 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456088467048_652635044",
      "id": "20160221-220107_1949634427",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eLittle helper for indicator codes\u003c/h3\u003e\n"
      },
      "dateCreated": "Feb 21, 2016 10:01:07 PM",
      "dateStarted": "Mar 7, 2016 9:15:47 PM",
      "dateFinished": "Mar 7, 2016 9:15:47 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql\n\nselect distinct IndicatorCode, IndicatorName from IndicatorsRDD\nwhere indicatorName like \"%unem%\"\norder by IndicatorCode",
      "dateUpdated": "Feb 26, 2016 12:03:49 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "IndicatorCode",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "IndicatorName",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "IndicatorCode",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "IndicatorName",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "editorMode": "ace/mode/sql"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455959691059_-678808695",
      "id": "20160220-101451_1095007117",
      "dateCreated": "Feb 20, 2016 10:14:51 AM",
      "dateStarted": "Feb 26, 2016 12:03:32 PM",
      "dateFinished": "Feb 26, 2016 12:03:35 PM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \n# (Lab 6) Spark Streaming\n\n## 6.1 Streaming from terminal \n\nSteps:\n- Open an ssh terminal to your cluster\n- Get the python code\n\n    ```bash\n    wget https://raw.githubusercontent.com/anset/SparkDemos/master/SimpleKafkaStreaming/direct_kafka_wordcount.py\n    ```\n\n- Look at [SimpleKafkaStreaming](https://github.com/anset/SparkDemos/tree/master/SimpleKafkaStreaming) and follow Steps 2 and 3",
      "dateUpdated": "Mar 7, 2016 9:15:48 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456470716626_1613381438",
      "id": "20160226-081156_1574950523",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch1\u003e(Lab 6) Spark Streaming\u003c/h1\u003e\n\u003ch2\u003e6.1 Streaming from terminal\u003c/h2\u003e\n\u003cp\u003eSteps:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003eOpen an ssh terminal to your cluster\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eGet the python code\u003c/p\u003e\n\u003cpre\u003e\u003ccode class\u003d\"bash\"\u003ewget https://raw.githubusercontent.com/anset/SparkDemos/master/SimpleKafkaStreaming/direct_kafka_wordcount.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003eLook at \u003ca href\u003d\"https://github.com/anset/SparkDemos/tree/master/SimpleKafkaStreaming\"\u003eSimpleKafkaStreaming\u003c/a\u003e and follow Steps 2 and 3\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Feb 26, 2016 8:11:56 AM",
      "dateStarted": "Mar 7, 2016 9:15:48 PM",
      "dateFinished": "Mar 7, 2016 9:15:48 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456481502522_-251613920",
      "id": "20160226-111142_591931088",
      "dateCreated": "Feb 26, 2016 11:11:42 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Spark Masterclass",
  "id": "2BB5HRGZ2",
  "angularObjects": {
    "2AXJWX8UE": [],
    "2AYJU33A2": [],
    "2AZHQMDRQ": [],
    "2AWXZJ28K": []
  },
  "config": {},
  "info": {}
}